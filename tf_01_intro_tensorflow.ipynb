{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 10:32:59.154066: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-24 10:32:59.203433: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz/micromamba/envs/ml/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.4729\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9543 - loss: 0.1553\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9663 - loss: 0.1114\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0863\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0728\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9693 - loss: 0.0969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08297765254974365, 0.9733999967575073]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# MNIST is a database of handwritten digits used for training and testing image processing systems and ML algorithms.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# The pixel values of the images range from 0 through 255. Scale to get a range of 0 to 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Each layer has one input tensor and one output tensor.\n",
    "# Layers have a known mathematical structure that can be reused and have trainable variables.\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax') # 10 classes for the ten digits 0-9\n",
    "])\n",
    "\n",
    "# Define a loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# configure and compile\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# fit model parameters to the data / minimize the loss:\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "# evaluate model's performance, on validation set.\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0832801e-11, 1.2580483e-08, 5.9691021e-09, 1.7820671e-02,\n",
       "        3.1533328e-18, 9.8217922e-01, 2.7335604e-15, 5.5547672e-10,\n",
       "        4.8505550e-11, 1.0238913e-07]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each example, the model returns a vector of logits or log-odds scores, one for each class.\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08555601 0.08555601 0.08555601 0.08709434 0.08555601 0.22845754\n",
      "  0.08555601 0.08555601 0.08555601 0.08555602]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.22845754, array([[0, 5]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tf.nn.softmax function converts these logits to probabilities for each class:\n",
    "import numpy\n",
    "\n",
    "# array of probabilities\n",
    "print(tf.nn.softmax(predictions).numpy())\n",
    "a = tf.nn.softmax(predictions).numpy()\n",
    "\n",
    "# max probability and position in the array\n",
    "a.max(), numpy.argwhere(a.max() == a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[0.08533861, 0.0853386 , 0.08533863, 0.08534613, 0.0853386 ,\n",
       "        0.08533864, 0.0853386 , 0.23193997, 0.08533883, 0.08534338],\n",
       "       [0.08536076, 0.08552364, 0.23158891, 0.08536174, 0.08536076,\n",
       "        0.08536103, 0.08536076, 0.08536076, 0.08536086, 0.08536076],\n",
       "       [0.08534589, 0.23182462, 0.08535514, 0.08534651, 0.08534698,\n",
       "        0.08534651, 0.08534652, 0.08537927, 0.08536269, 0.08534592],\n",
       "       [0.23191705, 0.08534005, 0.08534096, 0.08534006, 0.08534012,\n",
       "        0.08534006, 0.08534037, 0.0853461 , 0.08534006, 0.0853551 ],\n",
       "       [0.08536153, 0.08536142, 0.08536147, 0.08536142, 0.23157856,\n",
       "        0.08536142, 0.08536146, 0.08536289, 0.08536144, 0.08552845]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:\n",
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "probability_model(x_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
